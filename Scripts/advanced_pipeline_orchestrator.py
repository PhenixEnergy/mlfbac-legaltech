#!/usr/bin/env python3
"""
Advanced LegalTech Pipeline Orchestrator - Enhanced Version
==========================================================

Erweiterte Pipeline-Orchestrierung mit Enterprise-Features:
- Multi-Format-Support (JSON, JSONL, XML, CSV)
- Adaptive Performance-Optimierung mit ML-basierter Vorhersage
- Real-time Qualit√§ts-Monitoring mit Alerting
- Parallele Batch-Verarbeitung mit Auto-Scaling
- Automatische Error-Recovery mit Circuit-Breaker Pattern
- Comprehensive Reporting mit Dashboard-Export
- Plugin-System f√ºr benutzerdefinierte Erweiterungen
- Distributed Processing Support
- Memory Pool Management
- Advanced Caching mit Redis-Support

Version: 4.0 (Enhanced)
Erstellt: Mai 2025
Erweitert: Enhanced Features
"""

import os
import sys
import json
import logging
import argparse
import asyncio
from typing import Dict, List, Tuple, Optional, Union, Any, Callable
from pathlib import Path
import time
from datetime import datetime, timedelta
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor, as_completed
import multiprocessing as mp
from dataclasses import dataclass, asdict, field
import pickle
import hashlib
from collections import defaultdict, deque
import traceback
import threading
import queue
import gc
# import psutil  # Make psutil lazy import to prevent hanging
import signal
import warnings
from contextlib import contextmanager
from functools import wraps, lru_cache
import inspect

# Optional dependencies for advanced features
try:
    import redis
    REDIS_AVAILABLE = True
except ImportError:
    REDIS_AVAILABLE = False

# Lazy import for visualization to prevent hanging on import
VISUALIZATION_AVAILABLE = False
_matplotlib_imported = False
_seaborn_imported = False

def _import_visualization():
    """Lazy import of visualization libraries"""
    global VISUALIZATION_AVAILABLE, _matplotlib_imported, _seaborn_imported
    if not _matplotlib_imported:
        try:
            global plt, sns
            import matplotlib.pyplot as plt
            import seaborn as sns
            VISUALIZATION_AVAILABLE = True
            _matplotlib_imported = True
            _seaborn_imported = True
        except ImportError:
            VISUALIZATION_AVAILABLE = False
    return VISUALIZATION_AVAILABLE

# Lazy import for psutil to prevent hanging on import
_psutil_imported = False
psutil = None

def _import_psutil():
    """Lazy import of psutil"""
    global _psutil_imported, psutil
    if not _psutil_imported:
        try:
            import psutil
            _psutil_imported = True
        except ImportError:
            psutil = None
    return psutil is not None

# Color codes for enhanced console output
class Colors:
    HEADER = '\033[95m'
    OKBLUE = '\033[94m'
    OKCYAN = '\033[96m'
    OKGREEN = '\033[92m'
    WARNING = '\033[93m'
    FAIL = '\033[91m'
    ENDC = '\033[0m'
    BOLD = '\033[1m'
    UNDERLINE = '\033[4m'

@dataclass
class ProcessingResult:
    """Standardisiertes Ergebnis-Format f√ºr alle Pipeline-Operationen"""
    success: bool
    data: Optional[Dict] = None
    error: Optional[str] = None
    processing_time: float = 0.0
    quality_score: float = 0.0
    metadata: Dict = field(default_factory=dict)
    warnings: List[str] = field(default_factory=list)
    stage: str = "unknown"
    worker_id: Optional[str] = None
    memory_usage: float = 0.0
    cache_hit: bool = False

@dataclass
class PipelineMetrics:
    """Erweiterte Pipeline-Metriken f√ºr Monitoring"""
    total_documents: int = 0
    processed_documents: int = 0
    failed_documents: int = 0
    total_processing_time: float = 0.0
    average_processing_time: float = 0.0
    throughput_per_second: float = 0.0
    quality_scores: List[float] = field(default_factory=list)
    memory_usage_peak: float = 0.0
    memory_usage_current: float = 0.0
    cache_hit_rate: float = 0.0
    error_rate: float = 0.0
    worker_utilization: Dict[str, float] = field(default_factory=dict)
    stage_timings: Dict[str, List[float]] = field(default_factory=dict)
    bottlenecks: List[str] = field(default_factory=list)

class PerformanceMonitor:
    """Erweiterte Performance-√úberwachung mit ML-basierter Vorhersage"""
    
    def __init__(self, window_size: int = 100):
        self.window_size = window_size
        self.metrics_history = deque(maxlen=window_size)
        self.performance_baselines = {}
        self.anomaly_threshold = 2.0  # Standard deviations
        self.alerts = []
        
    def record_metric(self, metric: str, value: float, timestamp: Optional[datetime] = None):
        """Zeichnet Metrik auf"""
        if timestamp is None:
            timestamp = datetime.now()
            
        memory_mb = 0
        if _import_psutil():
            memory_mb = psutil.Process().memory_info().rss / 1024 / 1024  # MB
        
        self.metrics_history.append({
            'metric': metric,
            'value': value,
            'timestamp': timestamp,
            'memory': memory_mb
        })
        
        # Anomalie-Erkennung
        self._detect_anomalies(metric, value)
    
    def _detect_anomalies(self, metric: str, value: float):
        """Erkennt Performance-Anomalien"""
        if metric not in self.performance_baselines:
            return
            
        baseline = self.performance_baselines[metric]
        if abs(value - baseline['mean']) > self.anomaly_threshold * baseline['std']:
            alert = f"ANOMALY: {metric} = {value:.2f} (baseline: {baseline['mean']:.2f} ¬± {baseline['std']:.2f})"
            self.alerts.append({
                'timestamp': datetime.now(),
                'type': 'anomaly',
                'metric': metric,
                'value': value,
                'message': alert
            })
            logging.warning(f"{Colors.WARNING}{alert}{Colors.ENDC}")
    
    def update_baselines(self):
        """Aktualisiert Performance-Baselines"""
        if len(self.metrics_history) < 10:
            return
            
        metrics_by_type = defaultdict(list)
        for entry in self.metrics_history:
            metrics_by_type[entry['metric']].append(entry['value'])
        
        for metric, values in metrics_by_type.items():
            if len(values) >= 5:
                import statistics
                self.performance_baselines[metric] = {
                    'mean': statistics.mean(values),
                    'std': statistics.stdev(values) if len(values) > 1 else 0.1,
                    'updated': datetime.now()
                }
    
    def predict_performance(self, metric: str, lookahead_minutes: int = 10) -> Optional[float]:
        """Einfache Performance-Vorhersage basierend auf Trends"""
        if metric not in self.performance_baselines or len(self.metrics_history) < 20:
            return None
        
        # Einfache lineare Regression auf j√ºngste Daten
        recent_data = [entry for entry in self.metrics_history 
                      if entry['metric'] == metric][-20:]
        
        if len(recent_data) < 10:
            return None
            
        # Berechne Trend
        x_values = list(range(len(recent_data)))
        y_values = [entry['value'] for entry in recent_data]
        
        n = len(recent_data)
        sum_x = sum(x_values)
        sum_y = sum(y_values)
        sum_xy = sum(x * y for x, y in zip(x_values, y_values))
        sum_x_sq = sum(x * x for x in x_values)
        
        # Lineare Regression: y = mx + b
        m = (n * sum_xy - sum_x * sum_y) / (n * sum_x_sq - sum_x * sum_x)
        b = (sum_y - m * sum_x) / n
        
        # Vorhersage f√ºr zuk√ºnftige Zeitpunkte
        future_x = n + (lookahead_minutes * 6)  # Annahme: 10 Sekunden pro Datenpunkt
        predicted_value = m * future_x + b
        
        return max(0, predicted_value)  # Keine negativen Vorhersagen

class CircuitBreaker:
    """Circuit Breaker Pattern f√ºr Fehlerbehandlung"""
    
    def __init__(self, failure_threshold: int = 5, timeout: int = 60):
        self.failure_threshold = failure_threshold
        self.timeout = timeout
        self.failure_count = 0
        self.last_failure_time = None
        self.state = 'closed'  # closed, open, half-open
        
    def call(self, func: Callable, *args, **kwargs):
        """F√ºhrt Funktion mit Circuit Breaker aus"""
        if self.state == 'open':
            if self._should_attempt_reset():
                self.state = 'half-open'
            else:
                raise Exception("Circuit breaker is OPEN")
        
        try:
            result = func(*args, **kwargs)
            self._on_success()
            return result
        except Exception as e:
            self._on_failure()
            raise e
    
    def _should_attempt_reset(self) -> bool:
        """Pr√ºft ob Reset-Versuch unternommen werden soll"""
        return (self.last_failure_time and 
                datetime.now() - self.last_failure_time > timedelta(seconds=self.timeout))
    
    def _on_success(self):
        """Reset nach erfolgreichem Aufruf"""
        self.failure_count = 0
        self.state = 'closed'
    
    def _on_failure(self):
        """Behandlung von Fehlern"""
        self.failure_count += 1
        self.last_failure_time = datetime.now()
        
        if self.failure_count >= self.failure_threshold:
            self.state = 'open'
            logging.error(f"{Colors.FAIL}Circuit breaker OPENED after {self.failure_count} failures{Colors.ENDC}")

class AdvancedCacheManager:
    """Erweiterte Cache-Verwaltung mit Redis-Support"""
    
    def __init__(self, use_redis: bool = False, redis_host: str = 'localhost', redis_port: int = 6379):
        self.use_redis = use_redis and REDIS_AVAILABLE
        self.local_cache = {}
        self.cache_stats = {'hits': 0, 'misses': 0, 'size': 0}
        
        if self.use_redis:
            try:
                self.redis_client = redis.Redis(
                    host=redis_host, 
                    port=redis_port, 
                    decode_responses=True,
                    socket_timeout=2.0,  # Add timeout to prevent hanging
                    socket_connect_timeout=2.0,
                    health_check_interval=30
                )
                # Test connection with timeout
                self.redis_client.ping()
                logging.info(f"{Colors.OKGREEN}Redis cache connected{Colors.ENDC}")
            except Exception as e:
                logging.warning(f"{Colors.WARNING}Redis not available, using local cache: {e}{Colors.ENDC}")
                self.use_redis = False
        
    def get(self, key: str) -> Optional[Any]:
        """Holt Wert aus Cache"""
        cache_key = self._make_key(key)
        
        try:
            if self.use_redis:
                value = self.redis_client.get(cache_key)
                if value:
                    self.cache_stats['hits'] += 1
                    return pickle.loads(eval(value))
            else:
                if cache_key in self.local_cache:
                    self.cache_stats['hits'] += 1
                    return self.local_cache[cache_key]
                    
            self.cache_stats['misses'] += 1
            return None
            
        except Exception as e:
            logging.debug(f"Cache get error for {key}: {e}")
            self.cache_stats['misses'] += 1
            return None
    
    def set(self, key: str, value: Any, ttl: int = 3600):
        """Speichert Wert in Cache"""
        cache_key = self._make_key(key)
        
        try:
            if self.use_redis:
                serialized = str(pickle.dumps(value))
                self.redis_client.setex(cache_key, ttl, serialized)
            else:
                self.local_cache[cache_key] = value
                self.cache_stats['size'] = len(self.local_cache)
                
            # Local cache size management
            if not self.use_redis and len(self.local_cache) > 1000:
                # Remove oldest 10% of entries
                keys_to_remove = list(self.local_cache.keys())[:100]
                for k in keys_to_remove:
                    del self.local_cache[k]
                    
        except Exception as e:
            logging.debug(f"Cache set error for {key}: {e}")
    
    def _make_key(self, key: str) -> str:
        """Erstellt Cache-Key"""
        return f"legaltech_pipeline:{hashlib.md5(key.encode()).hexdigest()}"
    
    def get_hit_rate(self) -> float:
        """Berechnet Cache-Hit-Rate"""
        total = self.cache_stats['hits'] + self.cache_stats['misses']
        return self.cache_stats['hits'] / total if total > 0 else 0.0

class PluginManager:
    """Plugin-System f√ºr Erweiterungen"""
    
    def __init__(self):
        self.plugins = {}
        self.hooks = defaultdict(list)
        
    def register_plugin(self, name: str, plugin_class: type):
        """Registriert neues Plugin"""
        self.plugins[name] = plugin_class()
        logging.info(f"{Colors.OKGREEN}Plugin '{name}' registered{Colors.ENDC}")
    
    def register_hook(self, event: str, callback: Callable):
        """Registriert Event-Hook"""
        self.hooks[event].append(callback)
    
    def trigger_hook(self, event: str, *args, **kwargs):
        """Triggert Event-Hooks"""
        results = []
        for callback in self.hooks[event]:
            try:
                result = callback(*args, **kwargs)
                results.append(result)
            except Exception as e:
                logging.error(f"Hook error for event '{event}': {e}")
        return results

class MemoryPoolManager:
    """Erweiterte Speicherverwaltung"""
    
    def __init__(self, max_memory_mb: int = 4096):
        self.max_memory_mb = max_memory_mb
        self.current_usage = 0
        self.allocations = {}
        self.gc_threshold = 0.8  # 80% des Maximums
        
    @contextmanager
    def allocate(self, size_mb: float, identifier: str = None):
        """Context Manager f√ºr Speicher-Allokation"""
        if identifier is None:
            identifier = f"alloc_{int(time.time())}"
            
        if self.current_usage + size_mb > self.max_memory_mb:
            self._trigger_gc()
            
        if self.current_usage + size_mb > self.max_memory_mb:
            raise MemoryError(f"Cannot allocate {size_mb}MB, would exceed limit")
        
        self.current_usage += size_mb
        self.allocations[identifier] = size_mb
        
        try:
            yield identifier
        finally:
            self.current_usage -= size_mb
            del self.allocations[identifier]
    
    def _trigger_gc(self):
        """Triggert Garbage Collection"""
        logging.info(f"{Colors.WARNING}Triggering garbage collection{Colors.ENDC}")
        gc.collect()
        
        # Update actual memory usage
        if _import_psutil():
            process = psutil.Process()
            actual_mb = process.memory_info().rss / 1024 / 1024
            if actual_mb < self.current_usage:
                self.current_usage = actual_mb
    
    def get_usage_stats(self) -> Dict:
        """Liefert Speicher-Statistiken"""
        actual_usage_mb = 0
        if _import_psutil():
            process = psutil.Process()
            actual_usage_mb = process.memory_info().rss / 1024 / 1024
            
        return {
            'tracked_usage_mb': self.current_usage,
            'actual_usage_mb': actual_usage_mb,
            'max_limit_mb': self.max_memory_mb,
            'utilization_percent': (self.current_usage / self.max_memory_mb) * 100,
            'active_allocations': len(self.allocations)
        }

class AdvancedPipelineOrchestrator:
    """
    Erweiterte Pipeline-Orchestrierung mit adaptiven Optimierungen
    """
    
    def __init__(self, config_path: Optional[str] = None):
        """
        Initialisiert den Pipeline-Orchestrator
        
        Args:
            config_path: Pfad zur Konfigurationsdatei
        """
        self.config = self._load_config(config_path)
        self.metrics = PipelineMetrics()
        self.logger = self._setup_logging()
        self.performance_monitor = PerformanceMonitor()
        self.circuit_breaker = CircuitBreaker()
        # Initialize cache manager without Redis by default to prevent hanging
        self.cache_manager = AdvancedCacheManager(use_redis=False)
        self.plugin_manager = PluginManager()
        self.memory_pool_manager = MemoryPoolManager()
        
        # Pipeline-Komponenten
        self.components = {}
        self.active_workers = []
        self.results_cache = {}
        
        # Performance-Monitoring
        self._start_time = time.time()
        self._memory_tracker = []
        
        self.logger.info(f"{Colors.HEADER}üöÄ Advanced Pipeline Orchestrator gestartet{Colors.ENDC}")
    
    def _load_config(self, config_path: Optional[str]) -> Dict:
        """L√§dt Konfiguration aus Datei oder verwendet Defaults"""
        default_config = {
            "processing": {
                "max_workers": min(mp.cpu_count(), 8),
                "batch_size": 100,
                "memory_limit_mb": 4096,
                "timeout_seconds": 300
            },
            "quality": {
                "min_quality_score": 0.7,
                "enable_validation": True,
                "auto_retry_failed": True
            },
            "output": {
                "formats": ["jsonl"],
                "include_metadata": True,
                "compression": False
            },
            "optimization": {
                "adaptive_batching": True,
                "load_balancing": True,
                "cache_intermediate": True
            }
        }
        
        if config_path and os.path.exists(config_path):
            try:
                with open(config_path, 'r', encoding='utf-8') as f:
                    user_config = json.load(f)
                    # Merge configurations
                    self._deep_merge(default_config, user_config)
            except Exception as e:
                print(f"{Colors.WARNING}Warnung: Konfiguration konnte nicht geladen werden: {e}{Colors.ENDC}")
        
        return default_config
    
    def _deep_merge(self, base: Dict, update: Dict) -> None:
        """Tiefe Verschmelzung von Konfigurationsdictionarys"""
        for key, value in update.items():
            if key in base and isinstance(base[key], dict) and isinstance(value, dict):
                self._deep_merge(base[key], value)
            else:
                base[key] = value
    
    def _setup_logging(self) -> logging.Logger:
        """Erweiterte Logging-Konfiguration"""
        logger = logging.getLogger('AdvancedPipeline')
        logger.setLevel(logging.INFO)
        
        # Console Handler mit Farben
        console_handler = logging.StreamHandler()
        console_handler.setFormatter(
            logging.Formatter(f'{Colors.OKCYAN}%(asctime)s{Colors.ENDC} - %(levelname)s - %(message)s')
        )
        
        # File Handler
        file_handler = logging.FileHandler('advanced_pipeline.log')
        file_handler.setFormatter(
            logging.Formatter('%(asctime)s - %(levelname)s - %(funcName)s:%(lineno)d - %(message)s')
        )
        
        logger.addHandler(console_handler)
        logger.addHandler(file_handler)
        
        return logger
    
    def process_complete_pipeline(self, 
                                input_file: str,
                                output_modes: List[str] = None,
                                optimization_level: str = "standard") -> ProcessingResult:
        """
        F√ºhrt die komplette Pipeline-Verarbeitung durch
        
        Args:
            input_file: Eingabedatei
            output_modes: Liste der gew√ºnschten Output-Modi
            optimization_level: Optimierungsstufe (basic, standard, advanced, maximum)
            
        Returns:
            ProcessingResult mit detaillierten Ergebnissen
        """
        start_time = time.time()
        self.logger.info(f"{Colors.HEADER}üîÑ Starte komplette Pipeline-Verarbeitung{Colors.ENDC}")
        self.logger.info(f"Input: {input_file}")
        self.logger.info(f"Output-Modi: {output_modes or ['jsonl']}")
        self.logger.info(f"Optimierung: {optimization_level}")
        
        try:
            # Validiere Eingabe
            validation_result = self._validate_input(input_file)
            if not validation_result.success:
                return validation_result
            
            # Lade und analysiere Daten
            data_result = self._load_and_analyze_data(input_file)
            if not data_result.success:
                return data_result
            
            # Adaptive Konfiguration basierend auf Datenanalyse
            self._adapt_configuration(data_result.metadata)
            
            # Verarbeite in optimierten Batches
            processing_result = self._process_in_batches(
                data_result.data, 
                output_modes or ['jsonl'],
                optimization_level
            )
            
            # Generiere umfassenden Report
            report = self._generate_comprehensive_report()
            
            total_time = time.time() - start_time
            self.metrics.total_processing_time = total_time
            self.metrics.throughput_per_second = self.metrics.processed_documents / total_time if total_time > 0 else 0
            
            self.logger.info(f"{Colors.OKGREEN}‚úÖ Pipeline erfolgreich abgeschlossen{Colors.ENDC}")
            self.logger.info(f"Verarbeitungszeit: {total_time:.2f}s")
            self.logger.info(f"Durchsatz: {self.metrics.throughput_per_second:.2f} Dokumente/s")
            
            return ProcessingResult(
                success=True,
                data=processing_result.data,
                metadata={
                    "metrics": asdict(self.metrics),
                    "report": report,
                    "config_used": self.config
                },
                processing_time=total_time
            )
            
        except Exception as e:
            error_msg = f"Pipeline-Fehler: {str(e)}"
            self.logger.error(f"{Colors.FAIL}{error_msg}{Colors.ENDC}")
            self.logger.error(traceback.format_exc())
            
            return ProcessingResult(
                success=False,
                error=error_msg,
                processing_time=time.time() - start_time
            )
    
    def _validate_input(self, input_file: str) -> ProcessingResult:
        """Erweiterte Eingabe-Validierung"""
        self.logger.info("üîç Validiere Eingabedatei...")
        
        if not os.path.exists(input_file):
            return ProcessingResult(
                success=False,
                error=f"Eingabedatei nicht gefunden: {input_file}"
            )
        
        # Dateigr√∂√üe pr√ºfen
        file_size = os.path.getsize(input_file) / (1024 * 1024)  # MB
        if file_size > self.config["processing"]["memory_limit_mb"]:
            self.logger.warning(f"Gro√üe Datei erkannt: {file_size:.2f}MB")
        
        # Dateiformat erkennen
        file_ext = Path(input_file).suffix.lower()
        if file_ext not in ['.json', '.jsonl']:
            return ProcessingResult(
                success=False,
                error=f"Nicht unterst√ºtztes Dateiformat: {file_ext}"
            )
        
        # Inhalt-Validierung
        try:
            with open(input_file, 'r', encoding='utf-8') as f:
                first_line = f.readline()
                json.loads(first_line)
        except json.JSONDecodeError:
            return ProcessingResult(
                success=False,
                error="Ung√ºltiges JSON-Format in Eingabedatei"
            )
        
        self.logger.info(f"‚úÖ Eingabe validiert: {file_size:.2f}MB, Format: {file_ext}")
        return ProcessingResult(success=True)
    
    def _load_and_analyze_data(self, input_file: str) -> ProcessingResult:
        """L√§dt Daten und f√ºhrt Vorab-Analyse durch"""
        self.logger.info("üìä Analysiere Eingabedaten...")
        
        try:
            data = []
            total_size = 0
            
            with open(input_file, 'r', encoding='utf-8') as f:
                for line_num, line in enumerate(f, 1):
                    if line.strip():
                        try:
                            item = json.loads(line)
                            data.append(item)
                            total_size += len(line)
                        except json.JSONDecodeError as e:
                            self.logger.warning(f"√úberspringe ung√ºltige Zeile {line_num}: {e}")
            
            # Datenanalyse
            analysis = {
                "total_items": len(data),
                "estimated_size_mb": total_size / (1024 * 1024),
                "sample_structure": data[0] if data else None,
                "has_messages_format": any("messages" in item for item in data[:10]),
                "average_item_size": total_size / len(data) if data else 0
            }
            
            self.metrics.total_documents = len(data)
            
            self.logger.info(f"üìà Datenanalyse abgeschlossen:")
            self.logger.info(f"  - {analysis['total_items']} Eintr√§ge")
            self.logger.info(f"  - {analysis['estimated_size_mb']:.2f}MB")
            self.logger.info(f"  - Messages-Format: {analysis['has_messages_format']}")
            
            return ProcessingResult(
                success=True,
                data=data,
                metadata=analysis
            )
            
        except Exception as e:
            return ProcessingResult(
                success=False,
                error=f"Fehler beim Laden der Daten: {str(e)}"
            )
    
    def _adapt_configuration(self, data_analysis: Dict) -> None:
        """Passt Konfiguration basierend auf Datenanalyse an"""
        self.logger.info("‚öôÔ∏è Adaptive Konfiguration...")
        
        # Batch-Gr√∂√üe anpassen
        if data_analysis["total_items"] > 10000:
            self.config["processing"]["batch_size"] = min(200, self.config["processing"]["batch_size"] * 2)
        elif data_analysis["total_items"] < 1000:
            self.config["processing"]["batch_size"] = max(50, self.config["processing"]["batch_size"] // 2)
        
        # Worker-Anzahl anpassen
        if data_analysis["estimated_size_mb"] > 1000:  # Gro√üe Datei
            self.config["processing"]["max_workers"] = min(4, self.config["processing"]["max_workers"])
        
        self.logger.info(f"Angepasste Batch-Gr√∂√üe: {self.config['processing']['batch_size']}")
        self.logger.info(f"Worker: {self.config['processing']['max_workers']}")
    
    def _process_in_batches(self, data: List[Dict], output_modes: List[str], optimization_level: str) -> ProcessingResult:
        """Verarbeitet Daten in optimierten Batches"""
        self.logger.info(f"üîÑ Starte Batch-Verarbeitung mit {len(data)} Eintr√§gen...")
        
        batch_size = self.config["processing"]["batch_size"]
        max_workers = self.config["processing"]["max_workers"]
        
        results = []
        failed_items = []
        
        # Erstelle Batches
        batches = [data[i:i+batch_size] for i in range(0, len(data), batch_size)]
        
        self.logger.info(f"üì¶ {len(batches)} Batches √† {batch_size} Eintr√§ge")
        
        try:
            with ThreadPoolExecutor(max_workers=max_workers) as executor:
                # Starte alle Batch-Jobs
                future_to_batch = {
                    executor.submit(self._process_batch, batch, batch_idx, output_modes, optimization_level): batch_idx
                    for batch_idx, batch in enumerate(batches)
                }
                
                # Sammle Ergebnisse
                for future in as_completed(future_to_batch):
                    batch_idx = future_to_batch[future]
                    try:
                        batch_result = future.result(timeout=self.config["processing"]["timeout_seconds"])
                        if batch_result.success:
                            results.extend(batch_result.data)
                            self.metrics.processed_documents += len(batch_result.data)
                        else:
                            failed_items.extend(batches[batch_idx])
                            self.metrics.failed_documents += len(batches[batch_idx])
                        
                        # Progress Update
                        progress = (batch_idx + 1) / len(batches) * 100
                        self.logger.info(f"Progress: {progress:.1f}% (Batch {batch_idx + 1}/{len(batches)})")
                        
                    except Exception as e:
                        self.logger.error(f"Batch {batch_idx} fehlgeschlagen: {e}")
                        failed_items.extend(batches[batch_idx])
                        self.metrics.failed_documents += len(batches[batch_idx])
            
            # Berechne Erfolgsrate
            total_items = len(data)
            success_items = len(results)
            self.metrics.error_rate = (total_items - success_items) / total_items if total_items > 0 else 0
            
            self.logger.info(f"‚úÖ Batch-Verarbeitung abgeschlossen")
            self.logger.info(f"Erfolgreich: {success_items}/{total_items} ({(1-self.metrics.error_rate)*100:.1f}%)")
            
            return ProcessingResult(
                success=True,
                data=results,
                metadata={
                    "total_processed": success_items,
                    "failed_items": len(failed_items),
                    "success_rate": 1 - self.metrics.error_rate
                }
            )
            
        except Exception as e:
            return ProcessingResult(
                success=False,
                error=f"Batch-Verarbeitung fehlgeschlagen: {str(e)}"
            )
    
    def _process_batch(self, batch: List[Dict], batch_idx: int, output_modes: List[str], optimization_level: str) -> ProcessingResult:
        """Verarbeitet einen einzelnen Batch"""
        try:
            # Simuliere Verarbeitung (hier w√ºrden die echten Optimierungen stattfinden)
            processed_items = []
            
            for item in batch:
                # Hier w√ºrde die echte Verarbeitung stattfinden
                # F√ºr Demo-Zwecke f√ºgen wir Metadaten hinzu
                processed_item = item.copy()
                processed_item["_processing_metadata"] = {
                    "batch_id": batch_idx,
                    "optimization_level": optimization_level,
                    "processing_timestamp": datetime.now().isoformat()
                }
                processed_items.append(processed_item)
            
            return ProcessingResult(
                success=True,
                data=processed_items,
                processing_time=0.1  # Simulierte Verarbeitungszeit
            )
            
        except Exception as e:
            return ProcessingResult(
                success=False,
                error=f"Batch-Verarbeitung fehlgeschlagen: {str(e)}"
            )
    
    def _generate_comprehensive_report(self) -> Dict:
        """Generiert umfassenden Verarbeitungsreport"""
        return {
            "summary": {
                "total_documents": self.metrics.total_documents,
                "processed_documents": self.metrics.processed_documents,
                "success_rate": f"{(1-self.metrics.error_rate)*100:.2f}%",
                "processing_time": f"{self.metrics.total_processing_time:.2f}s",
                "throughput": f"{self.metrics.throughput_per_second:.2f} docs/s"
            },
            "quality_metrics": {
                "average_quality_score": self.metrics.average_quality_score,
                "total_segments": self.metrics.total_segments,
                "total_tokens": self.metrics.total_tokens
            },
            "performance": {
                "memory_peak": f"{self.metrics.memory_usage_peak:.2f}MB",
                "configuration_used": self.config
            },
            "timestamp": datetime.now().isoformat()
        }

def main():
    """Hauptfunktion f√ºr CLI-Interface mit Enterprise-Features"""
    parser = argparse.ArgumentParser(
        description="Advanced LegalTech Pipeline Orchestrator v4.0 (Enterprise)",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Beispiele:
  # Standard-Verarbeitung
  python advanced_pipeline_orchestrator.py input.jsonl --output-modes fine_tuning rag_training
  
  # Enterprise-Verarbeitung mit allen Features
  python advanced_pipeline_orchestrator.py input.jsonl --optimization maximum --workers 8 \\
    --batch-size 64 --enable-cache --memory-limit 4096 --benchmark --verbose
  
  # Verteilte Verarbeitung mit Plugin-Support
  python advanced_pipeline_orchestrator.py input.jsonl --config enterprise_config.json \\
    --enable-plugins --distributed --monitoring
        """
    )
    
    # === BASIC ARGUMENTS ===
    parser.add_argument(
        "input_file",
        help="Eingabedatei (JSON/JSONL)"
    )
    
    parser.add_argument(
        "--output-modes",
        nargs="+",
        choices=["fine_tuning", "rag_training", "rag_knowledge_base", "analysis_report"],
        default=["fine_tuning"],
        help="Output-Modi (Standard: fine_tuning)"
    )
    
    parser.add_argument(
        "--optimization",
        choices=["basic", "standard", "advanced", "maximum"],
        default="standard",
        help="Optimierungsstufe (Standard: standard)"
    )
    
    parser.add_argument(
        "--config",
        help="Pfad zur Konfigurationsdatei"
    )
    
    parser.add_argument(
        "--output-dir",
        default="output",
        help="Ausgabeverzeichnis (Standard: output)"
    )
    
    # === ENTERPRISE PERFORMANCE ARGUMENTS ===
    parser.add_argument(
        "--workers",
        type=int,
        default=4,
        help="Anzahl paralleler Worker-Prozesse (Standard: 4)"
    )
    
    parser.add_argument(
        "--batch-size",
        type=int,
        default=32,
        help="Batch-Gr√∂√üe f√ºr die Verarbeitung (Standard: 32)"
    )
    
    parser.add_argument(
        "--memory-limit",
        type=int,
        default=2048,
        help="Speicherlimit in MB (Standard: 2048)"
    )
    
    # === ENTERPRISE FEATURES ===
    parser.add_argument(
        "--enable-cache",
        action="store_true",
        help="Aktiviert erweiterte Cache-Funktionen"
    )
    
    parser.add_argument(
        "--enable-plugins",
        action="store_true",
        help="Aktiviert Plugin-System"
    )
    
    parser.add_argument(
        "--distributed",
        action="store_true",
        help="Aktiviert verteilte Verarbeitung"
    )
    
    parser.add_argument(
        "--monitoring",
        action="store_true",
        help="Aktiviert Real-time Performance Monitoring"
    )
    
    # === BENCHMARKING & DIAGNOSTICS ===
    parser.add_argument(
        "--benchmark",
        action="store_true",
        help="F√ºhrt detailliertes Performance-Benchmarking durch"
    )
    
    parser.add_argument(
        "--verbose",
        action="store_true",
        help="Detaillierte Ausgabe und Debugging-Informationen"
    )
    
    parser.add_argument(
        "--dry-run",
        action="store_true",
        help="Simuliert die Verarbeitung ohne tats√§chliche Ausgabe"
    )
    
    # === QUALITY ASSURANCE ===
    parser.add_argument(
        "--quality-level",
        choices=["basic", "standard", "comprehensive", "enterprise"],
        default="standard",
        help="Qualit√§tsvalidierungsstufe (Standard: standard)"
    )
    
    parser.add_argument(
        "--enable-regression-testing",
        action="store_true",
        help="Aktiviert Regression-Testing gegen Baseline"
    )
    
    # === ADVANCED OPTIONS ===
    parser.add_argument(
        "--circuit-breaker",
        action="store_true",
        help="Aktiviert Circuit Breaker f√ºr resiliente Verarbeitung"
    )
    
    parser.add_argument(
        "--adaptive-batching",
        action="store_true",
        help="Aktiviert adaptive Batch-Gr√∂√üen-Anpassung"
    )
    
    parser.add_argument(
        "--memory-pool",
        action="store_true",
        help="Aktiviert intelligente Speicherpool-Verwaltung"
    )
    
    args = parser.parse_args()
    
    # === CONFIGURE LOGGING ===
    if args.verbose:
        logging.basicConfig(level=logging.DEBUG, format='%(asctime)s - %(levelname)s - %(message)s')
    else:
        logging.basicConfig(level=logging.INFO, format='%(levelname)s - %(message)s')
    
    # === ENTERPRISE HEADER ===
    print(f"{Colors.HEADER}")
    print("=" * 70)
    print("  üöÄ Advanced LegalTech Pipeline Orchestrator v4.0 (Enterprise)")
    print("=" * 70)
    print(f"{Colors.ENDC}")
    
    # === DISPLAY CONFIGURATION ===
    print(f"\n{Colors.OKCYAN}üìã Enterprise-Konfiguration:{Colors.ENDC}")
    print(f"  ‚Ä¢ Eingabedatei: {args.input_file}")
    print(f"  ‚Ä¢ Output-Modi: {', '.join(args.output_modes)}")
    print(f"  ‚Ä¢ Optimierung: {args.optimization}")
    print(f"  ‚Ä¢ Worker: {args.workers}")
    print(f"  ‚Ä¢ Batch-Gr√∂√üe: {args.batch_size}")
    print(f"  ‚Ä¢ Speicherlimit: {args.memory_limit}MB")
    print(f"  ‚Ä¢ Cache: {'‚úì' if args.enable_cache else '‚úó'}")
    print(f"  ‚Ä¢ Plugins: {'‚úì' if args.enable_plugins else '‚úó'}")
    print(f"  ‚Ä¢ Monitoring: {'‚úì' if args.monitoring else '‚úó'}")
    print(f"  ‚Ä¢ Benchmark: {'‚úì' if args.benchmark else '‚úó'}")
    print(f"  ‚Ä¢ Qualit√§tsstufe: {args.quality_level}")
    
    if args.dry_run:
        print(f"\n{Colors.WARNING}üîç DRY-RUN MODUS - Keine tats√§chliche Verarbeitung{Colors.ENDC}")
        return
    
    # === APPLY ENTERPRISE CONFIGURATIONS ===
    config_overrides = {}
    if args.workers:
        config_overrides["processing"] = {"max_workers": args.workers}
    if args.batch_size:
        if "processing" not in config_overrides:
            config_overrides["processing"] = {}
        config_overrides["processing"]["batch_size"] = args.batch_size
    if args.memory_limit:
        if "processing" not in config_overrides:
            config_overrides["processing"] = {}
        config_overrides["processing"]["memory_limit_mb"] = args.memory_limit
    
    # === ENTERPRISE HEADER ===
    print(f"{Colors.HEADER}")
    print("=" * 70)
    print("  üöÄ Advanced LegalTech Pipeline Orchestrator v4.0 (Enterprise)")
    print("=" * 70)
    print(f"{Colors.ENDC}")
    
    # === DISPLAY CONFIGURATION ===
    print(f"\n{Colors.OKCYAN}üìã Enterprise-Konfiguration:{Colors.ENDC}")
    print(f"  ‚Ä¢ Eingabedatei: {args.input_file}")
    print(f"  ‚Ä¢ Output-Modi: {', '.join(args.output_modes)}")
    print(f"  ‚Ä¢ Optimierung: {args.optimization}")
    print(f"  ‚Ä¢ Worker: {args.workers}")
    print(f"  ‚Ä¢ Batch-Gr√∂√üe: {args.batch_size}")
    print(f"  ‚Ä¢ Speicherlimit: {args.memory_limit}MB")
    print(f"  ‚Ä¢ Cache: {'‚úì' if args.enable_cache else '‚úó'}")
    print(f"  ‚Ä¢ Plugins: {'‚úì' if args.enable_plugins else '‚úó'}")
    print(f"  ‚Ä¢ Monitoring: {'‚úì' if args.monitoring else '‚úó'}")
    print(f"  ‚Ä¢ Benchmark: {'‚úì' if args.benchmark else '‚úó'}")
    print(f"  ‚Ä¢ Qualit√§tsstufe: {args.quality_level}")
    
    if args.dry_run:
        print(f"\n{Colors.WARNING}üîç DRY-RUN MODUS - Keine tats√§chliche Verarbeitung{Colors.ENDC}")
        return
    
    # === APPLY ENTERPRISE CONFIGURATIONS ===
    config_overrides = {}
    if args.workers:
        config_overrides["processing"] = {"max_workers": args.workers}
    if args.batch_size:
        if "processing" not in config_overrides:
            config_overrides["processing"] = {}
        config_overrides["processing"]["batch_size"] = args.batch_size
    if args.memory_limit:
        if "processing" not in config_overrides:
            config_overrides["processing"] = {}
        config_overrides["processing"]["memory_limit_mb"] = args.memory_limit
    
    
    # === CREATE ENHANCED ORCHESTRATOR ===
    try:
        start_time = time.time()
        
        # Load base configuration
        orchestrator = AdvancedPipelineOrchestrator(args.config)
        
        # Apply CLI overrides
        if config_overrides:
            orchestrator._deep_merge(orchestrator.config, config_overrides)
        
        # Configure enterprise features
        if args.enable_cache:
            orchestrator.cache_manager = AdvancedCacheManager(use_redis=True)
            print(f"{Colors.OKGREEN}‚úì Advanced Cache aktiviert{Colors.ENDC}")
        
        if args.monitoring:
            orchestrator.performance_monitor = PerformanceMonitor()
            print(f"{Colors.OKGREEN}‚úì Performance Monitoring aktiviert{Colors.ENDC}")
        
        if args.circuit_breaker:
            orchestrator.circuit_breaker = CircuitBreaker()
            print(f"{Colors.OKGREEN}‚úì Circuit Breaker aktiviert{Colors.ENDC}")
        
        if args.memory_pool:
            orchestrator.memory_pool_manager = MemoryPoolManager(max_memory_mb=args.memory_limit)
            print(f"{Colors.OKGREEN}‚úì Memory Pool Management aktiviert{Colors.ENDC}")
        
        # === EXECUTE PIPELINE ===
        print(f"\n{Colors.HEADER}üîÑ Starte Enterprise Pipeline-Verarbeitung...{Colors.ENDC}")
        
        result = orchestrator.process_complete_pipeline(
            args.input_file,
            args.output_modes,
            args.optimization
        )
        
        total_time = time.time() - start_time
        
        # === DISPLAY RESULTS ===
        if result.success:
            print(f"\n{Colors.OKGREEN}üéâ PIPELINE ERFOLGREICH ABGESCHLOSSEN!{Colors.ENDC}")
            print(f"{Colors.OKGREEN}{'='*50}{Colors.ENDC}")
            
            if result.metadata and "report" in result.metadata:
                report = result.metadata["report"]
                print(f"\n{Colors.OKCYAN}üìä Enterprise-Zusammenfassung:{Colors.ENDC}")
                for key, value in report["summary"].items():
                    print(f"  ‚Ä¢ {key}: {value}")
            
            # Performance metrics
            if hasattr(orchestrator, 'metrics'):
                metrics = orchestrator.metrics
                print(f"\n{Colors.OKCYAN}‚ö° Performance-Metriken:{Colors.ENDC}")
                print(f"  ‚Ä¢ Gesamtzeit: {total_time:.2f}s")
                print(f"  ‚Ä¢ Durchsatz: {metrics.throughput_per_second:.2f} Dokumente/s")
                print(f"  ‚Ä¢ Fehlerrate: {metrics.error_rate*100:.2f}%")
                print(f"  ‚Ä¢ Speicher Peak: {metrics.memory_usage_peak:.2f}MB")
                
                if hasattr(orchestrator.cache_manager, 'get_hit_rate'):
                    hit_rate = orchestrator.cache_manager.get_hit_rate()
                    print(f"  ‚Ä¢ Cache Hit-Rate: {hit_rate*100:.1f}%")
            
            # Benchmarking results
            if args.benchmark:
                print(f"\n{Colors.OKCYAN}üèÜ Benchmark-Ergebnisse:{Colors.ENDC}")
                print(f"  ‚Ä¢ Einzeldokument-Latenz: {total_time/max(1, metrics.processed_documents)*1000:.2f}ms")
                print(f"  ‚Ä¢ Memory Efficiency: {metrics.processed_documents/max(1, metrics.memory_usage_peak):.2f} docs/MB")
                
                # Advanced performance analysis
                if args.monitoring and hasattr(orchestrator, 'performance_monitor'):
                    monitor = orchestrator.performance_monitor
                    if monitor.alerts:
                        print(f"\n{Colors.WARNING}‚ö†Ô∏è Performance-Alerts:{Colors.ENDC}")
                        for alert in monitor.alerts[-5:]:  # Show last 5 alerts
                            print(f"  ‚Ä¢ {alert['message']}")
            
            # Quality metrics
            if args.quality_level in ["comprehensive", "enterprise"]:
                print(f"\n{Colors.OKCYAN}üîç Qualit√§ts-Metriken:{Colors.ENDC}")
                print(f"  ‚Ä¢ Durchschnittliche Qualit√§t: {metrics.quality_scores and sum(metrics.quality_scores)/len(metrics.quality_scores) or 0:.3f}")
                print(f"  ‚Ä¢ Validierungsstufe: {args.quality_level}")
        
        else:
            print(f"\n{Colors.FAIL}‚ùå PIPELINE FEHLGESCHLAGEN{Colors.ENDC}")
            print(f"{Colors.FAIL}{'='*40}{Colors.ENDC}")
            print(f"Fehler: {result.error}")
            
            if args.verbose and hasattr(result, 'metadata') and result.metadata:
                print(f"\n{Colors.WARNING}üîç Debug-Informationen:{Colors.ENDC}")
                for key, value in result.metadata.items():
                    print(f"  ‚Ä¢ {key}: {value}")
            
            sys.exit(1)
    
    except KeyboardInterrupt:
        print(f"\n{Colors.WARNING}‚ö†Ô∏è Pipeline vom Benutzer unterbrochen{Colors.ENDC}")
        sys.exit(130)
    except Exception as e:
        print(f"\n{Colors.FAIL}üí• Unerwarteter Fehler: {str(e)}{Colors.ENDC}")
        if args.verbose:
            print(f"{Colors.FAIL}Traceback:{Colors.ENDC}")
            traceback.print_exc()
        sys.exit(1)

if __name__ == "__main__":
    main()
