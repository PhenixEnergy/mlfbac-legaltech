import requests
from bs4 import BeautifulSoup
import time
import chromadb
from chromadb.utils import embedding_functions
import hashlib
import os
import json
from urllib.parse import urljoin

# ========== KONFIGURATION - BITTE ANPASSEN ==========
BASE_URL = "https://www.dnoti.de/gutachten/"
INTERVAL = 3600  # Scan-Intervall in Sekunden (jede stunde)
DB_PATH = ""  # Pfad zur existierenden Vektordatenbank
STATE_FILE = ""  # Dateiname für den Scan-Zustand (z.B. "scanned_urls.json")
MAX_PAGES = 1 #erste seite der website

# Embedding-Modell konfigurieren
embedding_func = embedding_functions.SentenceTransformerEmbeddingFunction(
    model_name=""  # Modellname eintragen (z.B. "all-MiniLM-L6-v2")
)

# Header für Requests anpassen
HEADERS = {
    "User-Agent": "",  # Benutzeragent anpassen
    "Accept-Language": ""  # Spracheinstellungen
}
# ===================================================

# ChromaDB Client verbinden
chroma_client = chromadb.PersistentClient(path=DB_PATH)
collection = chroma_client.get_collection(
    name="",  # Name deiner Collection eintragen
    embedding_function=embedding_func
)

def load_scanned_urls():
    """Lädt bereits gescannte URLs aus der Zustandsdatei"""
    if not os.path.exists(STATE_FILE):
        return set()
    
    try:
        with open(STATE_FILE, 'r') as f:
            data = json.load(f)
            return set(data["urls"])
    except (json.JSONDecodeError, KeyError):
        return set()

def save_scanned_url(url):
    """Fügt eine neue URL zur Zustandsdatei hinzu"""
    urls = load_scanned_urls()
    urls.add(url)
    
    with open(STATE_FILE, 'w') as f:
        json.dump({"urls": list(urls)}, f, indent=2)

def extract_gutachten_data(soup, url):
    """Extrahiert Gutachten-Daten von Detailseite - CSS-Selektoren anpassen"""
    try:
        # ANPASSUNG: Titel-Selektor
        title = soup.select_one('').get_text(strip=True) if soup.select_one('') else "Ohne Titel"
        
        # ANPASSUNG: Aktenzeichen-Extraktion
        aktenzeichen = ""
        for element in soup.select(''):
            if 'Aktenzeichen' in element.text:
                aktenzeichen = element.get_text(strip=True).replace('Aktenzeichen:', '').strip()
                break
        
        # ANPASSUNG: Hauptinhalt-Selektor
        content_div = soup.select_one('')
        content = ""
        if content_div:
            # Entferne unerwünschte Elemente
            for element in content_div(['']):  # Elemente zum Entfernen eintragen
                element.decompose()
            
            # ANPASSUNG: Inhaltselemente auswählen
            content = "\n".join([p.get_text(strip=True) for p in content_div.select('')])
        
        return {
            "title": title,
            "aktenzeichen": aktenzeichen,
            "content": content,
            "url": url
        }
    except Exception as e:
        print(f"Extraktionsfehler {url}: {str(e)}")
        return None

def get_existing_ids():
    """Gibt alle existierenden IDs aus der Vektordatenbank zurück"""
    return set(collection.get(include=[])["ids"])

def process_list_page(page):
    """Verarbeitet eine Listenseite - CSS-Selektor für Links anpassen"""
    params = {
        # ANPASSUNG: Paginierungsparameter nach Bedarf anpassen
        "tx_dnotionlineplusapi_expertises[page]": page,
        # Weitere Parameter...
    }
    
    try:
        response = requests.get(BASE_URL, params=params, headers=HEADERS, timeout=15)
        response.raise_for_status()
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # ANPASSUNG: Selektor für Gutachten-Links
        gutachten_links = soup.select('')
        new_urls = []
        
        for link in gutachten_links:
            if link.get('href'):
                detail_url = urljoin(BASE_URL, link['href'])
                new_urls.append(detail_url)
        
        return new_urls
    except Exception as e:
        print(f"Fehler bei Seite {page}: {str(e)}")
        return []

def add_gutachten_to_db(gutachten_data):
    """Fügt ein Gutachten zur Vektordatenbank hinzu"""
    # Stabile ID aus URL und Aktenzeichen
    unique_id = hashlib.sha256(
        f"{gutachten_data['url']}_{gutachten_data['aktenzeichen']}".encode()
    ).hexdigest()[:20]
    
    # Prüfe ob bereits in der Datenbank
    existing_ids = get_existing_ids()
    if unique_id in existing_ids:
        return False
    
    # Füge zur Datenbank hinzu
    collection.add(
        documents=[gutachten_data["content"]],
        metadatas=[{
            "title": gutachten_data["title"],
            "aktenzeichen": gutachten_data["aktenzeichen"],
            "url": gutachten_data["url"],
            "source": "dnoti.de"
        }],
        ids=[unique_id]
    )
    
    # Markiere URL als verarbeitet
    save_scanned_url(gutachten_data["url"])
    return True

def scan_for_new_gutachten():
    """Scannt nach neuen Gutachten"""
    print("\n" + "="*50)
    print(f"Starte Scan: {time.strftime('%Y-%m-%d %H:%M:%S')}")
    
    scanned_urls = load_scanned_urls()
    new_items_count = 0
    
    for page in range(1, MAX_PAGES + 1):
        print(f"Scanne Seite {page}...")
        page_urls = process_list_page(page)
        
        for url in page_urls:
            if url in scanned_urls:
                continue
                
            try:
                print(f"Verarbeite: {url}")
                response = requests.get(url, headers=HEADERS, timeout=15)
                response.raise_for_status()
                
                soup = BeautifulSoup(response.text, 'html.parser')
                gutachten_data = extract_gutachten_data(soup, url)
                
                if not gutachten_data or not gutachten_data["content"]:
                    print(f"Kein Inhalt, überspringe: {url}")
                    save_scanned_url(url)
                    continue
                    
                if add_gutachten_to_db(gutachten_data):
                    new_items_count += 1
                    print(f"Neues Gutachten hinzugefügt: {gutachten_data['title']}")
                else:
                    print(f"Bereits vorhanden: {gutachten_data['title']}")
                
                time.sleep(0.3)
                
            except Exception as e:
                print(f"Fehler bei {url}: {str(e)}")
    
    print(f"Scan abgeschlossen. Neue Gutachten: {new_items_count}")
    return new_items_count

def initialize_scanned_urls():
    """Initialisiert die Zustandsdatei mit URLs aus der Datenbank"""
    try:
        items = collection.get(include=["metadatas"])
        urls = {m["url"] for m in items["metadatas"] if "url" in m}
        
        with open(STATE_FILE, 'w') as f:
            json.dump({"urls": list(urls)}, f, indent=2)
            
        print(f"Initialisiert mit {len(urls)} URLs aus DB")
    except Exception as e:
        print(f"Initialisierungsfehler: {str(e)}")
        with open(STATE_FILE, 'w') as f:
            json.dump({"urls": []}, f)

if __name__ == "__main__":
    print("DNOTI Gutachten Updater gestartet")
    print(f"Datenbankpfad: {DB_PATH}")
    print(f"Scan-Intervall: {INTERVAL//60} Minuten")
    
    if not os.path.exists(STATE_FILE):
        print("Initialisiere Zustandsdatei...")
        initialize_scanned_urls()
    
    while True:
        start_time = time.time()
        new_count = scan_for_new_gutachten()
        
        if new_count > 0:
            print(f"{new_count} neue Gutachten hinzugefügt")
        
        elapsed = time.time() - start_time
        sleep_time = max(10, INTERVAL - elapsed)
        
        print(f"Nächster Scan in {sleep_time//60:.0f} Minuten {sleep_time%60:.0f} Sekunden")
        time.sleep(sleep_time)